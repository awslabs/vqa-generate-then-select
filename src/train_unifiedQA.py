# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0

import time
import sys
import torch
import transformers
import numpy as np
from pathlib import Path
from torch.utils.data import DataLoader, RandomSampler, DistributedSampler, SequentialSampler
from src.options import Options

import src.slurm
import src.util
import src.evaluation
import src.data
import src.model
import os
import json
import pickle
from timm.utils import ModelEma


def train(model, model_ema, optimizer, scheduler, step, train_dataset, eval_dataset, opt, collator, best_dev_em, checkpoint_path):

  if opt.is_main:
    try:
      tb_logger = torch.utils.tensorboard.SummaryWriter(Path(opt.checkpoint_dir) / opt.name)
    except:
      tb_logger = None
      logger.warning('Tensorboard is not available.')

  torch.manual_seed(opt.global_rank + opt.seed)  #different seed for different sampling depending on global_rank
  train_sampler = RandomSampler(train_dataset)
  train_dataloader = DataLoader(train_dataset,
                                sampler=train_sampler,
                                batch_size=opt.per_gpu_batch_size,
                                drop_last=True,
                                num_workers=32,
                                collate_fn=collator)

  loss, curr_loss = 0.0, 0.0
  epoch = 1
  model.train()
  while step < opt.total_steps:
    epoch += 1
    for i, batch in enumerate(train_dataloader):
      step += 1
      (_, idx, labels, _, context_ids, context_mask) = batch
      # print(context_ids.shape)
      # print(context_ids)
      train_loss = model(input_ids=context_ids.cuda(), attention_mask=context_mask.cuda(), labels=labels.cuda())[0]

      train_loss.backward()

      if step % opt.accumulation_steps == 0:
        torch.nn.utils.clip_grad_norm_(model.parameters(), opt.clip)
        optimizer.step()
        scheduler.step()
        model.zero_grad()

      train_loss = src.util.average_main(train_loss, opt)
      curr_loss += train_loss.item()
      model_ema.update(model)

      if step % opt.eval_freq == 0:
        dev_em, result = evaluate(model, eval_dataset, tokenizer, collator, opt)
        model.train()
        if opt.is_main:
          if dev_em > best_dev_em:
            best_dev_em = dev_em
            src.util.save(model, optimizer, scheduler, step, best_dev_em, opt, checkpoint_path, 'best_dev')
            dst_path = os.path.join(checkpoint_path, 'checkpoint')
            with open(os.path.join(dst_path, 'best_result.pkl'), 'wb') as output:
              pickle.dump(result, output)
          log = f"{step} / {opt.total_steps} |"
          log += f"train: {curr_loss/opt.eval_freq:.3f} |"
          log += f"evaluation: {100*dev_em:.2f} EM |"
          log += f"lr: {scheduler.get_last_lr()[0]:.5f}"
          logger.info(log)
          curr_loss = 0
          if tb_logger is not None:
            tb_logger.add_scalar("Evaluation", dev_em, step)
            tb_logger.add_scalar("Training", curr_loss / (opt.eval_freq), step)

      if opt.is_main and step % opt.save_freq == 0:
        src.util.save(model, optimizer, scheduler, step, best_dev_em, opt, checkpoint_path, f"step-{step}")
      if step > opt.total_steps:
        break


def evaluate(model, dataset, tokenizer, collator, opt):
  sampler = SequentialSampler(dataset)
  dataloader = DataLoader(dataset, sampler=sampler, batch_size=opt.per_gpu_batch_size, drop_last=False, num_workers=16, collate_fn=collator)
  model.eval()
  acc = []
  result = {}
  model = model.module if hasattr(model, "module") else model
  with torch.no_grad():
    for i, batch in enumerate(dataloader):
      (img_id, idx, _, _, context_ids, context_mask) = batch

      outputs = model.generate(input_ids=context_ids.cuda(), attention_mask=context_mask.cuda(), max_length=10)

      for k, o in enumerate(outputs):
        ans = tokenizer.decode(o, skip_special_tokens=True)
        gold = dataset.get_example(idx[k])['answers']
        # print(ans, gold)
        result[img_id[k]] = (ans, gold)
        cur_acc = src.evaluation.okvqa_ems(ans, gold)
        acc.append(cur_acc)

  accuracy = sum(acc) / len(acc)
  result['accuracy'] = accuracy
  return accuracy, result


if __name__ == "__main__":
  options = Options()
  opt = options.parse()
  arg_folder = os.path.join(opt.checkpoint_dir, opt.name)
  with open(os.path.join(opt.checkpoint_dir, 'args.txt'), 'w') as f:
    json.dump(opt.__dict__, f, indent=2)

  torch.manual_seed(opt.seed)
  src.slurm.init_distributed_mode(opt)
  src.slurm.init_signal_handler()

  checkpoint_path = Path(opt.checkpoint_dir) / opt.name
  checkpoint_exists = False

  if opt.is_distributed:
    torch.distributed.barrier()
  checkpoint_path.mkdir(parents=True, exist_ok=True)

  logger = src.util.init_logger(opt.is_main, opt.is_distributed, checkpoint_path / 'run.log')

  str_args = ' '.join(f'{k}={v}' for k, v in vars(opt).items())
  logger.info(str_args)

  model_name = f'allenai/unifiedqa-v2-t5-{opt.model_size}-1251000'
  model_class = transformers.T5ForConditionalGeneration

  #load data
  tokenizer = transformers.T5Tokenizer.from_pretrained(model_name)

  collator = src.data.OKvqaCollatorUnifiedQA(opt.text_maxlength, tokenizer, answer_maxlength=opt.answer_maxlength)
  # use golbal rank and world size to split the eval set on multiple gpus
  train_examples = src.data.load_unifiedQA_data(opt.train_data,
                                                split_type='train2014',
                                                global_rank=opt.global_rank,
                                                world_size=opt.world_size,
                                                gpt_name=opt.gpt_name)

  train_dataset = src.data.OkvqaDatasetUnifiedQA(train_examples, opt.n_context)

  # use golbal rank and world size to split the eval set on multiple gpus
  eval_examples = src.data.load_unifiedQA_data(opt.eval_data,
                                               split_type='val2014',
                                               global_rank=opt.global_rank,
                                               world_size=opt.world_size,
                                               gpt_name=opt.gpt_name)
  eval_dataset = src.data.OkvqaDatasetUnifiedQA(eval_examples, opt.n_context)

  if not checkpoint_exists and opt.model_path == "none":
    model = transformers.T5ForConditionalGeneration.from_pretrained(model_name)
    model = model.to(opt.local_rank)
    optimizer, scheduler = src.util.set_optim(opt, model)
    step, best_dev_em = 0, 0.0
  elif opt.model_path == "none":
    load_path = checkpoint_path / 'checkpoint' / 'latest'
    model, optimizer, scheduler, opt_checkpoint, step, best_dev_em = \
        src.util.load(model_class, load_path, opt, reset_params=False)
    logger.info(f"Model loaded from {load_path}")
  else:
    model, optimizer, scheduler, opt_checkpoint, step, best_dev_em = \
        src.util.load(model_class, opt.model_path, opt, reset_params=True)
    logger.info(f"Model loaded from {opt.model_path}")

  # model.set_checkpoint(opt.use_checkpoint)

  model_ema_decay = 0.99996
  model_ema_force_cpu = False
  model_ema = ModelEma(model, decay=model_ema_decay, device='cpu' if model_ema_force_cpu else '', resume='')

  if opt.is_distributed:
    model = torch.nn.parallel.DistributedDataParallel(
        model,
        device_ids=[opt.local_rank],
        output_device=opt.local_rank,
        find_unused_parameters=False,
    )

  logger.info("Start training")
  train(model, model_ema, optimizer, scheduler, step, train_dataset, eval_dataset, opt, collator, best_dev_em, checkpoint_path)
